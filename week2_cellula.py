# -*- coding: utf-8 -*-
"""Week2_Cellula.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hptCdQpJ_Z6etJ9jbzTmO2Q5gdlPeXtq

# **Addressing Large Model Size with Quantization**

**1. Problem: Large Model Size in LLMs**
Models like BERT (110M parameters) and LLaMA (7B â€“ 65B parameters) are huge, which leads to:

High storage cost (several GBs).

Large memory footprint during inference.

Slow deployment on resource-limited devices (e.g., edge, mobile).

For example:

BERT-base (110M params Ã— 32-bit float) â†’ ~420 MB

LLaMA-65B (~65 billion params Ã— 32-bit float) â†’ ~260 GB

Clearly, deploying such models is impractical without compression.

**2. Quantization as a Solution**

Quantization reduces model size by lowering the precision of weights/activations (e.g., FP32 â†’ INT8/INT4).

 Mathematical Idea

Given a real-valued weight
ğ‘¤
âˆˆ
ğ‘…
wâˆˆR, quantization maps it to a smaller discrete set:

ğ‘„
(
ğ‘¤
)
=
round
(
ğ‘¤
âˆ’
min
â¡
(
ğ‘Š
)
ğ‘ 
)
â‹…
ğ‘ 
+
min
â¡
(
ğ‘Š
)
Q(w)=round(
s
wâˆ’min(W)
	â€‹

)â‹…s+min(W)

where:

ğ‘Š
W = set of weights

ğ‘ 
=
max
â¡
(
ğ‘Š
)
âˆ’
min
â¡
(
ğ‘Š
)
2
ğ‘
âˆ’
1
s=
2
b
âˆ’1
max(W)âˆ’min(W)
	â€‹

 is the scale factor

ğ‘
b = number of bits (e.g., 8 for INT8)

This reduces storage but may cause a small accuracy drop.

**3. Types of Quantization**

Post-Training Quantization (PTQ): Quantize after training. Fast, but accuracy may degrade.

Quantization-Aware Training (QAT): Simulates quantization during training â†’ better accuracy.

Dynamic Quantization: Quantize only weights, compute activations in FP32.

Mixed Precision (FP16/INT8): Use different precisions for different layers.

**Example: BERT Quantization in PyTorch**
"""

import torch
from transformers import BertForSequenceClassification

# Load pretrained BERT model
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")

print("Original model size (MB):", model.num_parameters() * 4 / 1e6)

# Apply Dynamic Quantization
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear},  # layers to quantize
    dtype=torch.qint8   # target precision
)

print("Quantized model size (MB):", quantized_model.num_parameters() * 1 / 1e6)

"""**Quantization-Aware Training (QAT) Example (PyTorch)**"""

import torch
import torch.quantization as quant

# Define a simple model
class SimpleModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = torch.nn.Linear(128, 64)
        self.relu = torch.nn.ReLU()
        self.fc2 = torch.nn.Linear(64, 10)

    def forward(self, x):
        return self.fc2(self.relu(self.fc1(x)))

# Prepare for QAT
model = SimpleModel()
model.qconfig = quant.get_default_qat_qconfig("fbgemm")
quant.prepare_qat(model, inplace=True)

# Simulate training
x = torch.randn(8, 128)
y = model(x)

# Convert to quantized model
quantized_model = quant.convert(model.eval(), inplace=False)

print(quantized_model)

"""**Graphical Illustration (Weight Distribution)**"""

import matplotlib.pyplot as plt
import numpy as np

# Simulated weights
weights = np.random.normal(0, 1, 10000)

# Quantize to INT8
min_w, max_w = weights.min(), weights.max()
scale = (max_w - min_w) / (2**8 - 1)
quantized = np.round((weights - min_w) / scale) * scale + min_w

plt.hist(weights, bins=50, alpha=0.5, label="FP32 Weights")
plt.hist(quantized, bins=50, alpha=0.5, label="INT8 Weights")
plt.legend()
plt.title("Effect of Quantization on Weight Distribution")
plt.show()

"""**Real-World Examples**

BERT INT8 (Intel OpenVINO, PyTorch) â†’ up to 4Ã— faster inference with <1% accuracy drop.

LLaMA-7B 4-bit quantization (QLoRA) â†’ reduces memory from ~32GB â†’ ~4GB, enabling fine-tuning on a single GPU.

**Conclusion**

Quantization is a powerful technique to:

-Reduce model size by 2Ã— to 8Ã—.

-Accelerate inference, especially on CPUs.

-Enable deployment of massive LLMs (like LLaMA) on consumer GPUs or edge devices.

However:

-Accuracy may slightly degrade.

-Careful selection of quantization scheme (PTQ vs QAT) is crucial.
"""

import matplotlib.pyplot as plt
import numpy as np

# Number of parameters
models = {
    "BERT-base": 110e6,
    "LLaMA-7B": 7e9,
    "LLaMA-65B": 65e9
}

# Bytes per parameter
sizes = {
    "FP32": 4,
    "FP16": 2,
    "INT8": 1,
    "INT4": 0.5
}

def compute_size(params, precision_bytes):
    return params * precision_bytes / 1e6  # MB

precisions = list(sizes.keys())
x = np.arange(len(models))

plt.figure(figsize=(12,6))

for i, prec in enumerate(precisions):
    sizes_mb = [compute_size(p, sizes[prec]) for p in models.values()]
    bars = plt.bar(x + i*0.2, sizes_mb, width=0.2, label=prec)

    # Add value labels on top of bars
    for bar, val in zip(bars, sizes_mb):
        if val > 1000:  # show in GB if large
            label = f"{val/1024:.1f} GB"
        else:
            label = f"{val:.0f} MB"
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), label,
                 ha='center', va='bottom', fontsize=8)

plt.xticks(x + 0.3, models.keys())
plt.ylabel("Model Size (MB)")
plt.title("Comparison of Model Sizes with Different Quantization Techniques")
plt.legend()
plt.show()